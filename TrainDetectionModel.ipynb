{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Our prototype model for our live detection model ##\n",
        "\n",
        "This model is currenrtly being tweaked to detect hands (gloves or no gloves). This model will be pipelined to our classification model to determine whether or not the detected subject is wearing gloves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoidXkk_GNKn",
        "outputId": "549ccd6c-e394-4a39-8536-fe94c53cae03"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynRf3PfHHDGY",
        "outputId": "b6261b0d-90f4-4052-fce0-e9183fd384c6"
      },
      "outputs": [],
      "source": [
        "# Mount the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXxiVYrCJHMS",
        "outputId": "4706b80f-4a22-43c3-c797-24181c3aac8f"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/SCAI_Neural_Network_Project/yolo_dataset\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejWFbbySIa9f"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import io\n",
        "import numpy as np\n",
        "from tensorflow.keras.saving import register_keras_serializable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtiVmDQIIgVO"
      },
      "outputs": [],
      "source": [
        "train_labels_directory = \"labels/combined_train\"\n",
        "train_images_directory = \"images/combined_train\"\n",
        "\n",
        "val_labels_directory = \"labels/combined_valid\"\n",
        "val_images_directory = \"images/combined_valid\"\n",
        "\n",
        "test_labels_directory = \"labels/combined_test\"\n",
        "test_images_directory = \"images/combined_test\"\n",
        "\n",
        "LABEL_MAP_DICT ={\n",
        "    0: \"no_gloves\",\n",
        "    1: \"has_gloves\"\n",
        "    }\n",
        "\n",
        "IMAGE_FORMAT = \"jpg\"\n",
        "IMAGE_FORMAT_2 = \"png\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding our dataset ##\n",
        "\n",
        "In order for Tensorflow to read our data, we create a binary representation of our data encapsulating all of our features including the images, the text labels which contain all the of the bounding box and class information. It is stored in form of `.tfrecord`. By saving it, we can simply load the test, train, and val data if we do not include any new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPIPqoyLujyk"
      },
      "outputs": [],
      "source": [
        "def create_tf_example(image_directory, label_directory, format):\n",
        "  with tf.io.gfile.GFile(os.path.join(image_directory), 'rb') as fid:\n",
        "    encoded_jpg = fid.read()\n",
        "  image = Image.open(io.BytesIO(encoded_jpg))\n",
        "  width, height = image.size\n",
        "  filename = image_directory.split('/')[-1].encode('utf8')\n",
        "  image_format = \"\"\n",
        "  if format== IMAGE_FORMAT:\n",
        "    image_format = IMAGE_FORMAT.encode('utf8')\n",
        "  elif format == IMAGE_FORMAT_2:\n",
        "    image_format = IMAGE_FORMAT_2.encode('utf8')\n",
        "  else:\n",
        "    raise ValueError(\"Image format not recognized.\")\n",
        "  xmins = []\n",
        "  xmaxs = []\n",
        "  ymins = []\n",
        "  ymaxs = []\n",
        "  classes_text = []\n",
        "  classes = []\n",
        "  with open(label_directory, 'r') as label_file:\n",
        "    for line in label_file.readlines():\n",
        "      class_id, x_center, y_center, bound_width, bound_height = map(float, line.strip().split())\n",
        "      xmin = (x_center - bound_width / 2) * width\n",
        "      xmax = (x_center + bound_width / 2) * width\n",
        "      ymin = (y_center - bound_height / 2) * height\n",
        "      ymax = (y_center + bound_height / 2) * height\n",
        "\n",
        "      xmins.append(xmin / width)\n",
        "      xmaxs.append(xmax / width)\n",
        "      ymins.append(ymin / height)\n",
        "      ymaxs.append(ymax / height)\n",
        "\n",
        "      class_id = int(class_id)\n",
        "      classes_text.append(LABEL_MAP_DICT[class_id].encode('utf8'))\n",
        "      classes.append(class_id + 1)\n",
        "\n",
        "  tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
        "        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
        "        'image/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
        "        'image/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
        "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_jpg])),\n",
        "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n",
        "        'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmins)),\n",
        "        'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmaxs)),\n",
        "        'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymins)),\n",
        "        'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymaxs)),\n",
        "        'image/object/class/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=classes_text)),\n",
        "        'image/object/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
        "        }))\n",
        "  return tf_example\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0D0UF3RwzEDB",
        "outputId": "e6e400a7-6cba-428b-9c0e-7d6e04c475e3"
      },
      "outputs": [],
      "source": [
        "# CREATE RECORDS FOR TRAINING DATA\n",
        "##########################################################\n",
        "\n",
        "#For each file, create an encoded representating incorporating both the image and the bounding labels\n",
        "writer = tf.io.TFRecordWriter(\"combined_train.tfrecord\")\n",
        "\n",
        "intended_directory = train_labels_directory\n",
        "\n",
        "num_files = len(os.listdir(intended_directory))\n",
        "index = 0\n",
        "\n",
        "#pick which image/ label to use: train, val, or test.\n",
        "for label_file in os.listdir(intended_directory):\n",
        "  label_path = os.path.join(intended_directory, label_file)\n",
        "  if not label_file.endswith(\".txt\"):\n",
        "    continue\n",
        "  image_file_jpg = os.path.splitext(label_file)[0] + \".\" + IMAGE_FORMAT\n",
        "  image_path_jpg = os.path.join(train_images_directory, image_file_jpg)\n",
        "\n",
        "  image_file_png = os.path.splitext(label_file)[0] + \".\" + IMAGE_FORMAT_2\n",
        "  image_path_png = os.path.join(train_images_directory, image_file_png)\n",
        "\n",
        "  label_path = os.path.join(train_labels_directory, label_file)\n",
        "\n",
        "  format = IMAGE_FORMAT\n",
        "  if os.path.exists(image_path_jpg):\n",
        "    image_path = image_path_jpg\n",
        "  elif os.path.exists(image_path_png):\n",
        "    image_path = image_path_png\n",
        "    format = IMAGE_FORMAT_2\n",
        "  else:\n",
        "    print(f\"Image file not found: {image_path_jpg} or {image_path_png}\")\n",
        "    continue\n",
        "\n",
        "\n",
        "  tf_example = create_tf_example(image_path, label_path, format)\n",
        "  writer.write(tf_example.SerializeToString())\n",
        "  index += 1\n",
        "  print(f\"Processed {index}/{num_files} files\") if index % 10 == 0 else None\n",
        "\n",
        "writer.close()\n",
        "print(\"tf record saved to combined_train.tfrecord\")\n",
        "\n",
        "##########################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# CREATE RECORDS FOR VALIDATION DATA\n",
        "##########################################################\n",
        "\n",
        "#For each file, create an encoded representating incorporating both the image and the bounding labels\n",
        "writer = tf.io.TFRecordWriter(\"combined_val.tfrecord\")\n",
        "\n",
        "intended_directory = val_labels_directory\n",
        "\n",
        "num_files = len(os.listdir(intended_directory))\n",
        "index = 0\n",
        "\n",
        "#pick which image/ label to use: train, val, or test.\n",
        "for label_file in os.listdir(intended_directory):\n",
        "  label_path = os.path.join(val_labels_directory, label_file)\n",
        "  if not label_file.endswith(\".txt\"):\n",
        "    continue\n",
        "  image_file = os.path.splitext(label_file)[0] + \".\" + IMAGE_FORMAT\n",
        "  image_path = os.path.join(val_images_directory, image_file)\n",
        "  label_path = os.path.join(val_labels_directory, label_file)\n",
        "\n",
        "\n",
        "  if not os.path.exists(image_path):\n",
        "    print(f\"Image file not found: {image_path}\")\n",
        "    continue\n",
        "\n",
        "  tf_example = create_tf_example(image_path, label_path)\n",
        "  writer.write(tf_example.SerializeToString())\n",
        "  index += 1\n",
        "  print(f\"Processed {index}/{num_files} files\") if index % 10 == 0 else None\n",
        "\n",
        "writer.close()\n",
        "print(\"tf record saved to combined_val.tfrecord\")\n",
        "\n",
        "##########################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# CREATE RECORDS FOR TEST DATA\n",
        "##########################################################\n",
        "\n",
        "#For each file, create an encoded representating incorporating both the image and the bounding labels\n",
        "writer = tf.io.TFRecordWriter(\"combined_test.tfrecord\")\n",
        "\n",
        "intended_directory = test_labels_directory\n",
        "\n",
        "num_files = len(os.listdir(intended_directory))\n",
        "index = 0\n",
        "\n",
        "#pick which image/ label to use: train, val, or test.\n",
        "for label_file in os.listdir(intended_directory):\n",
        "  label_path = os.path.join(test_labels_directory, label_file)\n",
        "  if not label_file.endswith(\".txt\"):\n",
        "    continue\n",
        "  image_file = os.path.splitext(label_file)[0] + \".\" + IMAGE_FORMAT\n",
        "  image_path = os.path.join(test_images_directory, image_file)\n",
        "  label_path = os.path.join(test_labels_directory, label_file)\n",
        "\n",
        "\n",
        "  if not os.path.exists(image_path):\n",
        "    print(f\"Image file not found: {image_path}\")\n",
        "    continue\n",
        "\n",
        "  tf_example = create_tf_example(image_path, label_path)\n",
        "  writer.write(tf_example.SerializeToString())\n",
        "  index += 1\n",
        "  print(f\"Processed {index}/{num_files} files\")\n",
        "\n",
        "writer.close()\n",
        "print(\"tf record saved to combined_test.tfrecord\")\n",
        "\n",
        "##########################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iZGnkGhJPxr"
      },
      "outputs": [],
      "source": [
        "def parse_tfrecord(record_input):\n",
        "  feature = {\n",
        "      'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
        "      'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
        "      'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
        "      'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
        "      'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
        "      'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32)\n",
        "        }\n",
        "  parsed_record = tf.io.parse_single_example(record_input, feature)\n",
        "  print(parsed_record)\n",
        "\n",
        "  image = tf.image.decode_jpeg(parsed_record['image/encoded'],channels=3)\n",
        "  image = tf.image.resize(image, [256, 256]) / 255.0\n",
        "\n",
        "  bounding_boxes = tf.stack([\n",
        "    tf.sparse.to_dense(parsed_record['image/object/bbox/ymin']),\n",
        "    tf.sparse.to_dense(parsed_record['image/object/bbox/xmin']),\n",
        "    tf.sparse.to_dense(parsed_record['image/object/bbox/ymax']),\n",
        "    tf.sparse.to_dense(parsed_record['image/object/bbox/xmax'])\n",
        "  ], axis=1)\n",
        "\n",
        "\n",
        "  return image, bounding_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmGMfcK3Kq5F"
      },
      "outputs": [],
      "source": [
        "def load_dataset(record_path, batch_size, shuffle = False):\n",
        "  dataset = tf.data.TFRecordDataset(record_path)\n",
        "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(512)\n",
        "  dataset = dataset.padded_batch(batch_size, padded_shapes=([256, 256, 3], [None, 4]))\n",
        "  dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFtZWcouPG1d",
        "outputId": "4fedde77-346c-4350-b8a4-ee21b630d993"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(\"combined_train.tfrecord\", 16, shuffle = True)\n",
        "val_dataset = load_dataset(\"combined_val.tfrecord\", 16)\n",
        "test_dataset = load_dataset(\"combined_test.tfrecord\", 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Our Detection Model ##\n",
        "This is the core part of our live detection model, which includes 6 convolutional layers to extract the features, 2 dropout layers to discourage overfitting, and 2 MaxPooling layers to reduce the spatial dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwSTS0RHMsEo"
      },
      "outputs": [],
      "source": [
        "@register_keras_serializable()\n",
        "class detector(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(detector, self).__init__()\n",
        "    self.backbone = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, 2, strides = 1, activation = 'relu', padding= 'same'),\n",
        "        #tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Conv2D(64, 2, strides = 1, activation = 'relu', padding= 'same'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Conv2D(128, 2, strides = 1, activation = 'relu', padding= 'same'),\n",
        "        #tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Conv2D(256, 2, strides = 1, activation = 'relu', padding= 'same'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Dropout(0.50),\n",
        "        tf.keras.layers.Conv2D(512, 2, strides = 1, activation = 'relu', padding= 'same'),\n",
        "        #tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Conv2D(1024, 2, strides = 1, activation = 'relu', padding= 'same'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.GlobalAveragePooling2D()\n",
        "    ])\n",
        "    self.box_head = tf.keras.layers.Dense(4, activation = 'sigmoid')\n",
        "  def call(self, x):\n",
        "    return self.box_head(self.backbone(x))\n",
        "\n",
        "def loss_fn(bboxes_true, box_preds):\n",
        "    return tf.reduce_mean(tf.square(bboxes_true[:, 0] - box_preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kjt69CrkO96c",
        "outputId": "b1221fa5-841d-4d7b-e86d-774eca9b4abd"
      },
      "outputs": [],
      "source": [
        "## Train the model\n",
        "\n",
        "model = detector()\n",
        "model.compile(optimizer = 'adam', loss = loss_fn)\n",
        "\n",
        "num_images = 3875\n",
        "batch_size = 16\n",
        "steps_per_epoch = num_images // batch_size\n",
        "\n",
        "model.fit(train_dataset,validation_data=val_dataset, epochs = 70, steps_per_epoch = steps_per_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jI5FeBayAt6",
        "outputId": "855fb9c7-98f8-4a5f-bf3c-69d1c836cd1d"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "\n",
        "model.export(\"new_model1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing our model ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaO1oQlLcvus",
        "outputId": "22eb1700-827f-4e81-f754-fe297e204172"
      },
      "outputs": [],
      "source": [
        "# Test model we just trained.\n",
        "\n",
        "# note: add more dropouts. looks like the model is starting to memorize the the data\n",
        "print(\"evaluating the dataset\")\n",
        "result = model.evaluate(test_dataset)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUQ-WUJhIv-v"
      },
      "outputs": [],
      "source": [
        "# Compute the accuracy of the model using iou loss on test dataset\n",
        "\n",
        "\"\"\"\n",
        "def compute_dist(box1, box2):\n",
        "  ymin1, xmin1, ymax1, xmax1 = box1\n",
        "  ymin2, xmin2, ymax2, xmax2 = box2\n",
        "  print(f\"computing differences between {box1} and {box2}.\")\n",
        "\n",
        "  dist = (xmax1 - xmin1) * (ymax1 - ymin1) + (xmax2 - xmin2) * (ymax2 - ymin2) - (xmax2 - xmin2) * (ymax2 - ymin2)\n",
        "\n",
        "  return dist\n",
        "\"\"\"\n",
        "\n",
        "def compute_iou(box1, box2):\n",
        "  ymin1, xmin1, ymax1, xmax1 = box1\n",
        "  ymin2, xmin2, ymax2, xmax2 = box2\n",
        "\n",
        "  xi1 = max(xmin1, xmin2)\n",
        "  yi1 = max(ymin1, ymin2)\n",
        "  xi2 = min(xmax1, xmax2)\n",
        "  yi2 = min(ymax1, ymax2)\n",
        "\n",
        "  intersection_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
        "\n",
        "  box1_area = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
        "  box2_area = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
        "\n",
        "  union_area = box1_area + box2_area - intersection_area\n",
        "\n",
        "  iou = intersection_area / union_area if union_area > 0 else 0.0\n",
        "  \n",
        "  return iou\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP9nL9tMOn9E",
        "outputId": "977f28cc-523c-4252-bc0d-29d7834832cc"
      },
      "outputs": [],
      "source": [
        "# Test model with test images.\n",
        "\n",
        "num_images = len(os.listdir(test_images_directory))\n",
        "computed_ious = 0.0\n",
        "index = 0\n",
        "for test_image in os.listdir(test_images_directory):\n",
        "  if (test_image.endswith(\".png\")) or (test_image.endswith(\".jpg\")) or (test_image.endswith(\".jpeg\")):\n",
        "    None\n",
        "  else:\n",
        "    continue\n",
        "  img = Image.open(os.path.join(test_images_directory, test_image)).convert(\"RGB\")\n",
        "  img = img.resize((256,256))\n",
        "  input_tensor = tf.convert_to_tensor(np.array(img)/255.0, dtype = tf.float32)\n",
        "  input_tensor = tf.expand_dims(input_tensor, axis = 0)\n",
        "\n",
        "  pred_bounds = model(input_tensor)\n",
        "  pred_bounds = pred_bounds[0].numpy()\n",
        "\n",
        "  img_width, img_height = img.size\n",
        "  ymin, xmin, ymax, xmax = pred_bounds\n",
        "\n",
        "  (ymin, xmin, ymax, xmax) = (int(ymin * img_height), int(xmin * img_width), int(ymax * img_height), int(xmax * img_width))\n",
        "\n",
        "  normalized_bounds = (ymin, xmin, ymax, xmax)\n",
        "\n",
        "  test_labels = test_image.replace(\".jpg\", \".txt\")\n",
        "  if test_labels not in os.listdir(test_labels_directory):\n",
        "    test_labels = test_image.replace(\".jpeg\", \".txt\")\n",
        "\n",
        "  label_directory = os.path.join(test_labels_directory, test_labels)\n",
        "  #print(label_directory)\n",
        "\n",
        "  with open(label_directory, 'r') as label_file:\n",
        "    for line in label_file.readlines():\n",
        "      class_id, x_center, y_center, bound_width, bound_height = map(float, line.strip().split())\n",
        "      xmin_actual = (x_center - bound_width / 2) * img_width\n",
        "      xmax_actual = (x_center + bound_width / 2) * img_width\n",
        "      ymin_actual = (y_center - bound_height / 2) * img_height\n",
        "      ymax_actual = (y_center + bound_height / 2) * img_height\n",
        "      actual_bounds =  (int(ymin_actual), int(xmin_actual), int(ymax_actual), int(xmax_actual))\n",
        "      iou = compute_iou(normalized_bounds,actual_bounds)\n",
        "      #print(f\"Current IOU: {iou * 100:.2f}%\")\n",
        "      computed_ious += iou\n",
        "      index += 1\n",
        "average_iou = computed_ious / index\n",
        "print(f\"\\nAverage IOU: {average_iou * 100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
